{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanaSrinitha/Mohana_INF05731_Spring2024/blob/main/Shaga_Mohana_Exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Bag of Words (BoW): Bag of Words (BoW) is an easy but effective text analysis method that ignores word order and syntax in favor of word frequency.\n",
        "Because of its ease of use and effectiveness, BoW is frequently utilized in natural language processing applications like text classification, sentiment analysis, and information retrieval. It establishes how frequently a word occurs in the text.\n",
        "Term Frequency-Inverse Document Frequency, or TF-IDF, compares a word's frequency in all texts to determine how significant it is in a given document.\n",
        "Part-of-Speech (POS) Tagging- This method extracts the POS tags for every word and, depending on specific POS patterns, can yield sentiment signals. This algorithm makes use of nltk, which gives information about the grammatical categories of words.\n",
        "Sentiment Lexicon Score: This method uses a sentiment lexicon to give words a sentiment score, which is then added together to create the overall sentiment score for the document. The code determines sentiment scores for every sentence in the text_data using TextBlob.\n",
        "N-grams:Words, characters, or symbols that are taken from a particular text or voice and arranged in a contiguous sequence are referred to as n-grams.\"n\": Indicates how many elements there are in each sequence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPyUdP3SiPcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install nltk scikit-learn"
      ],
      "metadata": {
        "id": "ZLSER8JIgzuG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50b337c-c6b6-4669-d711-249d9ceaa641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW) Features:\n",
            "[[1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0]\n",
            " [0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
            " [0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0]]\n",
            "\n",
            "TF-IDF Features:\n",
            "[[0.43671931 0.34431452 0.         0.         0.         0.43671931\n",
            "  0.         0.         0.         0.43671931 0.43671931 0.\n",
            "  0.         0.         0.         0.         0.34431452 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.38166888 0.         0.         0.\n",
            "  0.38166888 0.         0.         0.         0.         0.38166888\n",
            "  0.         0.         0.         0.30091213 0.30091213 0.\n",
            "  0.38166888 0.38166888 0.30091213 0.        ]\n",
            " [0.         0.         0.         0.         0.4472136  0.\n",
            "  0.         0.4472136  0.4472136  0.         0.         0.\n",
            "  0.         0.         0.4472136  0.         0.         0.\n",
            "  0.         0.         0.         0.4472136 ]\n",
            " [0.         0.32555709 0.         0.41292788 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.41292788 0.41292788 0.         0.32555709 0.         0.41292788\n",
            "  0.         0.         0.32555709 0.        ]]\n",
            "\n",
            "Sentiment Lexicon Scores:\n",
            "[0.5750000000000001, 0.2, 1.0, -0.3]\n",
            "\n",
            "Bigrams Features:\n",
            "[[1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0]\n",
            " [0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
            " [0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0]]\n",
            "\n",
            "POS Tags:\n",
            "[[('This', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('fantastic', 'JJ'), ('and', 'CC'), ('amazing', 'JJ'), ('!', '.')], [('The', 'DT'), ('food', 'NN'), ('at', 'IN'), ('this', 'DT'), ('restaurant', 'NN'), ('was', 'VBD'), ('very', 'RB'), ('unhygenic', 'JJ'), ('.', '.')], [('I', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('wonderful', 'JJ'), ('experience', 'NN'), ('shopping', 'NN'), ('here', 'RB'), ('.', '.')], [('The', 'DT'), ('customer', 'NN'), ('service', 'NN'), ('was', 'VBD'), ('rude', 'JJ'), ('and', 'CC'), ('unhelpful', 'JJ'), ('.', '.')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download 'punkt' and 'averaged_perceptron_tagger' resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample data\n",
        "text_data = [\n",
        "    \"The film was fantastic and really impressive.\",\n",
        "    \"The food at this restaurant was very unhygenic.\",\n",
        "    \"It was a lovely shopping experience for me.\",\n",
        "    \"The customer service provided was rude and unhelpful..\"\n",
        "]\n",
        "\n",
        "# Bag of Words (BoW) feature extraction\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_features = bow_vectorizer.fit_transform(text_data)\n",
        "print(\"Bag of Words (BoW) Features:\")\n",
        "print(bow_features.toarray())\n",
        "\n",
        "# TF-IDF feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
        "print(\"\\nTF-IDF Features:\")\n",
        "print(tfidf_features.toarray())\n",
        "\n",
        "\n",
        "# Sentiment Lexicon feature extraction\n",
        "sentiment_scores = []\n",
        "for sentence in text_data:\n",
        "    sentiment = TextBlob(sentence).sentiment.polarity\n",
        "    sentiment_scores.append(sentiment)\n",
        "print(\"\\nSentiment Lexicon Scores:\")\n",
        "print(sentiment_scores)\n",
        "\n",
        "# N-grams feature extraction (bigrams)\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_features = ngram_vectorizer.fit_transform(text_data)\n",
        "print(\"\\nBigrams Features:\")\n",
        "print(ngram_features.toarray())\n",
        "\n",
        "# 5. Part-of-Speech (POS) Tagging\n",
        "pos_tags = [pos_tag(word_tokenize(text)) for text in text_data]\n",
        "print(\"\\nPOS Tags:\")\n",
        "print(pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd83b2aa-eda8-4222-eca6-6277b5b4a810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features based on Importance:\n",
            "1. is: 0.6882472016116852\n",
            "2. this: 0.4588314677411235\n",
            "3. docnument: 0.22941573387056174\n",
            "4. document: 0.22941573387056174\n",
            "5. documentthis: 0.22941573387056174\n",
            "6. second: 0.22941573387056174\n",
            "7. the: 0.22941573387056174\n",
            "8. third: 0.22941573387056174\n"
          ]
        }
      ],
      "source": [
        "# Feature Selection using Filter, Wrapper, and Hybrid Methods with TF-IDF Feature\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    'This is the document.'\n",
        "    'This is second document'\n",
        "    'this is third docnument',\n",
        "]\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Rank features based on importance (descending order)\n",
        "feature_importance = {feature: importance for feature, importance in zip(feature_names, X.toarray()[0])}\n",
        "sorted_features = sorted(feature_importance, key=feature_importance.get, reverse=True)\n",
        "\n",
        "# Print ranked features\n",
        "print(\"Ranked Features based on Importance:\")\n",
        "for idx, feature in enumerate(sorted_features):\n",
        "    print(f\"{idx+1}. {feature}: {feature_importance[feature]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajtMjzGVityF",
        "outputId": "95af1e12-dff6-4b2f-fed5-ab11f477fac8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.5.0-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c8b50a-8ddf-463b-828e-a9f9de88d3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Text Based on Cosine Similarity:\n",
            "1. The film was fantastic and really impressive. (Cosine Similarity: 0.7100)\n",
            "2. It was a lovely shopping experience for me. (Cosine Similarity: 0.1689)\n",
            "3. The food at this restaurant was very unhygienic. (Cosine Similarity: -0.0563)\n",
            "4. The customer service provided was rude and unhelpful. (Cosine Similarity: -0.1406)\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample data\n",
        "text_data = [\n",
        "    \"The film was fantastic and really impressive.\",\n",
        "    \"The food at this restaurant was very unhygienic.\",\n",
        "    \"It was a lovely shopping experience for me.\",\n",
        "    \"The customer service provided was rude and unhelpful.\"\n",
        "]\n",
        "\n",
        "# BERT model for sentence embeddings\n",
        "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Query\n",
        "query = \"A great movie with impressive acting.\"\n",
        "\n",
        "# Calculate BERT embeddings for text data and the query\n",
        "text_embeddings = bert_model.encode(text_data, convert_to_tensor=True)\n",
        "query_embedding = bert_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# Reshape the query_embedding to make it a 2D array\n",
        "query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "cosine_similarities = cosine_similarity(query_embedding, text_embeddings).flatten()\n",
        "\n",
        "# Rank the text based on cosine similarity in descending order\n",
        "ranked_indices = cosine_similarities.argsort()[::-1]\n",
        "\n",
        "# Print the ranked text\n",
        "print(\"Ranked Text Based on Cosine Similarity:\")\n",
        "for i, index in enumerate(ranked_indices):\n",
        "    print(f\"{i + 1}. {text_data[index]} (Cosine Similarity: {cosine_similarities[index]:.4f})\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AvLgTPIiV0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}