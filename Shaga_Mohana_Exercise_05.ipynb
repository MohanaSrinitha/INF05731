{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohanaSrinitha/Mohana_INF05731_Spring2024/blob/main/Shaga_Mohana_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn import naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def read_data(file_path):\n",
        "    text_data, sentiments = [], []\n",
        "    file_data = open(file_path).read()\n",
        "    for i, j in enumerate(file_data.split(\"\\n\")):\n",
        "        after_split = j.split(' ')\n",
        "        text_data.append(\" \".join(after_split[1:]))\n",
        "        sentiments.append(after_split[0])\n",
        "    return text_data, sentiments\n",
        "\n",
        "\n",
        "def preprocess_data(data_frame):\n",
        "    # Convert sentiment classes to integers\n",
        "    data_frame['Sentimental Value'] = pd.to_numeric(data_frame['Sentimental Value'], errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values in the target variable\n",
        "    data_frame = data_frame.dropna(subset=['Sentimental Value'])\n",
        "\n",
        "    # Pre-processing\n",
        "    # removal of special characters\n",
        "    data_frame['After noise removal'] = data_frame['Raw Data'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', char) for char in x))\n",
        "\n",
        "    # removal of Punctuation\n",
        "    data_frame['Punctuation removal'] = data_frame['After noise removal'].str.replace('[^\\w\\s]', '')\n",
        "\n",
        "    # Stopwords removal\n",
        "    stop_word = stopwords.words('english')\n",
        "    data_frame['Stopwords removal'] = data_frame['Punctuation removal'].apply(\n",
        "        lambda x: \" \".join(word for word in x.split() if word not in stop_word))\n",
        "\n",
        "    # Lower Casing\n",
        "    data_frame['Lower casing'] = data_frame['Stopwords removal'].apply(lambda x: \" \".join(word.lower() for word in x.split()))\n",
        "\n",
        "    return data_frame\n",
        "\n",
        "\n",
        "def vectorize_data(train_df, test_df):\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vector = TfidfVectorizer(analyzer='word')\n",
        "    tfidf_vector.fit(train_df['Lower casing'])\n",
        "    x_train = tfidf_vector.transform(train_df['Lower casing'])\n",
        "    tfidf_vector_test = TfidfVectorizer(analyzer='word', vocabulary=tfidf_vector.vocabulary_)\n",
        "    tfidf_vector_test.fit(test_df['Lower casing'])\n",
        "    x_test = tfidf_vector_test.transform(test_df['Lower casing'])\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "\n",
        "def evaluate_classifier(classifier, x_train, y_train, x_test, y_test):\n",
        "    classifier.fit(x_train, y_train)\n",
        "    predicted = classifier.predict(x_test)\n",
        "    accuracy = accuracy_score(predicted, y_test)\n",
        "    print(f\"Accuracy of Training data ({classifier.__class__.__name__}): {accuracy}\")\n",
        "\n",
        "    predicted_testing = classifier.predict(x_test)\n",
        "    accuracy_testing = accuracy_score(predicted_testing, y_test)\n",
        "    print(f\"Accuracy of Testing data ({classifier.__class__.__name__}): {accuracy_testing}\")\n",
        "\n",
        "    if 'XGB' not in str(classifier):\n",
        "        scoring = 'accuracy'\n",
        "        kfold = KFold(10, random_state=7, shuffle=True)\n",
        "        cross_val = cross_val_score(classifier, x_test, y_test, cv=kfold, scoring=scoring).mean()\n",
        "        print(f\"Cross-validation score ({classifier.__class__.__name__}): {cross_val}\")\n",
        "\n",
        "    print(f\"\\nClassification Report ({classifier.__class__.__name__}):\\n\")\n",
        "    print(classification_report(y_test, predicted))\n",
        "\n",
        "\n",
        "# Read data\n",
        "training_text_data, training_sentiments = read_data('stsa-train.txt')\n",
        "testing_text_data, testing_sentiments = read_data('stsa-test.txt')\n",
        "\n",
        "# Create data frames\n",
        "training_df = pd.DataFrame(list(zip(training_sentiments, training_text_data)), columns=['Sentimental Value', 'Raw Data'])\n",
        "testing_df = pd.DataFrame(list(zip(testing_sentiments, testing_text_data)), columns=['Sentimental Value', 'Raw Data'])\n",
        "\n",
        "# Pre-process data\n",
        "training_df = preprocess_data(training_df)\n",
        "testing_df = preprocess_data(testing_df)\n",
        "\n",
        "# Vectorize data\n",
        "x_train, x_test = vectorize_data(training_df, testing_df)\n",
        "\n",
        "# Split data for training and testing\n",
        "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train, training_df['Sentimental Value'],\n",
        "                                                                      test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure target variables are integers\n",
        "y_train = y_train.astype(int)\n",
        "y_valid = y_valid.astype(int)\n",
        "\n",
        "# List of classifiers\n",
        "classifiers = [\n",
        "    naive_bayes.MultinomialNB(),\n",
        "    svm.SVC(),\n",
        "    KNeighborsClassifier(n_neighbors=5),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    XGBClassifier()\n",
        "]\n",
        "\n",
        "# Evaluate each classifier\n",
        "for classifier in classifiers:\n",
        "    print(f\"Evaluating {classifier.__class__.__name__}:\")\n",
        "    evaluate_classifier(classifier, x_train, y_train, x_valid, y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq2wiUH3jKBa",
        "outputId": "29a75e39-3ce2-4d45-a4d2-d284ed1e0466"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating MultinomialNB:\n",
            "Accuracy of Training data (MultinomialNB): 0.7911849710982659\n",
            "Accuracy of Testing data (MultinomialNB): 0.7911849710982659\n",
            "Cross-validation score (MultinomialNB): 0.7015848191012408\n",
            "\n",
            "Classification Report (MultinomialNB):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.69      0.76       671\n",
            "           1       0.75      0.89      0.81       713\n",
            "\n",
            "    accuracy                           0.79      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.79      0.79      1384\n",
            "\n",
            "Evaluating SVC:\n",
            "Accuracy of Training data (SVC): 0.796242774566474\n",
            "Accuracy of Testing data (SVC): 0.796242774566474\n",
            "Cross-validation score (SVC): 0.7008758210822645\n",
            "\n",
            "Classification Report (SVC):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.73      0.78       671\n",
            "           1       0.77      0.86      0.81       713\n",
            "\n",
            "    accuracy                           0.80      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.80      0.80      1384\n",
            "\n",
            "Evaluating KNeighborsClassifier:\n",
            "Accuracy of Training data (KNeighborsClassifier): 0.523121387283237\n",
            "Accuracy of Testing data (KNeighborsClassifier): 0.523121387283237\n",
            "Cross-validation score (KNeighborsClassifier): 0.6062350119904077\n",
            "\n",
            "Classification Report (KNeighborsClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.52      0.51       671\n",
            "           1       0.54      0.53      0.53       713\n",
            "\n",
            "    accuracy                           0.52      1384\n",
            "   macro avg       0.52      0.52      0.52      1384\n",
            "weighted avg       0.52      0.52      0.52      1384\n",
            "\n",
            "Evaluating DecisionTreeClassifier:\n",
            "Accuracy of Training data (DecisionTreeClassifier): 0.684971098265896\n",
            "Accuracy of Testing data (DecisionTreeClassifier): 0.684971098265896\n",
            "Cross-validation score (DecisionTreeClassifier): 0.6012668126368471\n",
            "\n",
            "Classification Report (DecisionTreeClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.70      0.68       671\n",
            "           1       0.70      0.67      0.69       713\n",
            "\n",
            "    accuracy                           0.68      1384\n",
            "   macro avg       0.69      0.69      0.68      1384\n",
            "weighted avg       0.69      0.68      0.69      1384\n",
            "\n",
            "Evaluating RandomForestClassifier:\n",
            "Accuracy of Training data (RandomForestClassifier): 0.75\n",
            "Accuracy of Testing data (RandomForestClassifier): 0.75\n",
            "Cross-validation score (RandomForestClassifier): 0.6466478990720467\n",
            "\n",
            "Classification Report (RandomForestClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.68      0.72       671\n",
            "           1       0.73      0.82      0.77       713\n",
            "\n",
            "    accuracy                           0.75      1384\n",
            "   macro avg       0.75      0.75      0.75      1384\n",
            "weighted avg       0.75      0.75      0.75      1384\n",
            "\n",
            "Evaluating XGBClassifier:\n",
            "Accuracy of Training data (XGBClassifier): 0.6986994219653179\n",
            "Accuracy of Testing data (XGBClassifier): 0.6986994219653179\n",
            "\n",
            "Classification Report (XGBClassifier):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.53      0.63       671\n",
            "           1       0.66      0.86      0.75       713\n",
            "\n",
            "    accuracy                           0.70      1384\n",
            "   macro avg       0.72      0.69      0.69      1384\n",
            "weighted avg       0.72      0.70      0.69      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence=sentence.replace('{html}',\"\")\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)\n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kYTPPTorhdJ",
        "outputId": "d867c82f-475f-4f94-ad42-1cc83b930dbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package bcp47 to /root/nltk_data...\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package extended_omw to /root/nltk_data...\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pe08 to /root/nltk_data...\n",
            "       |   Unzipping corpora/pe08.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Package stopwords is already up-to-date!\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Package wordnet is already up-to-date!\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet2022 to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet2022.zip.\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | \n",
            "     Done downloading collection all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "df['Reviews']=df['Reviews'].map(lambda s:data_preprocess(s))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "tCTbA17y8LvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF VECTORIZATION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "tfidf_vects = tfidf_vect.fit_transform(df['Reviews'].values.astype('U'))\n",
        "names= tfidf_vect.get_feature_names()"
      ],
      "metadata": {
        "id": "gmhklbY4s8oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ELBOW METHOD\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(2,12):\n",
        "    kmeans = KMeans(n_clusters = i, init = \"k-means++\", random_state = 101)\n",
        "    kmeans.fit(tfidf_vects)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize = (11,6))\n",
        "plt.plot(range(2,12), wcss, marker = \"o\")\n",
        "plt.title (\"The Elbow Method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"WCSS\")"
      ],
      "metadata": {
        "id": "1qt0LrfUo0Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forming 6 clusters\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters = 6,init='k-means++',max_iter=10000, random_state=50)\n",
        "model.fit(tfidf_vects)\n",
        "from collections import Counter\n",
        "Counter(model.labels_)"
      ],
      "metadata": {
        "id": "2I-MvymY8QxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clusters containing words with maximum strength\n",
        "top_words = 7\n",
        "centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "for cluster_num in range(6):\n",
        "    key_features = [names[i] for i in centroids[cluster_num, :top_words]]\n",
        "    print('Cluster '+str(cluster_num+1))\n",
        "    print('Top Words:', key_features)"
      ],
      "metadata": {
        "id": "A0D0GpC6m-3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_center=model.cluster_centers_\n",
        "cluster_center"
      ],
      "metadata": {
        "id": "0WMHJn4snBW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews=[]\n",
        "for i in df['Reviews']:\n",
        "    reviews.append(str(i).split())\n",
        "import gensim\n",
        "w2v_model=gensim.models.Word2Vec(reviews, size=100, workers=4)\n",
        "\n",
        "import numpy as np\n",
        "vectors = []\n",
        "for i in reviews:\n",
        "    vector = np.zeros(100)\n",
        "    count = 0\n",
        "    for word in i:\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            vector += vec\n",
        "            count += 1\n",
        "        except:\n",
        "            pass\n",
        "    vector /= count\n",
        "    vectors.append(vector)\n",
        "vectors = np.array(vectors)\n",
        "vectors = np.nan_to_num(vectors)"
      ],
      "metadata": {
        "id": "cVeOxiq9nESd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_center=model.cluster_centers_\n",
        "cluster_center"
      ],
      "metadata": {
        "id": "Z8hAPsUwnGQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews=[]\n",
        "for i in df['Reviews']:\n",
        "    reviews.append(str(i).split())\n",
        "import gensim\n",
        "w2v_model=gensim.models.Word2Vec(reviews, size=100, workers=4)\n",
        "\n",
        "import numpy as np\n",
        "vectors = []\n",
        "for i in reviews:\n",
        "    vector = np.zeros(100)\n",
        "    count = 0\n",
        "    for word in i:\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            vector += vec\n",
        "            count += 1\n",
        "        except:\n",
        "            pass\n",
        "    vector /= count\n",
        "    vectors.append(vector)\n",
        "vectors = np.array(vectors)\n",
        "vectors = np.nan_to_num(vectors)"
      ],
      "metadata": {
        "id": "8R5nJwKQnIuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "minPts = 2 * 100\n",
        "# Lower bound function\n",
        "def lower_bound(nums, target):\n",
        "    l, r = 0, len(nums) - 1\n",
        "    # Binary searching\n",
        "    while l <= r:\n",
        "        mid = int(l + (r - l) / 2)\n",
        "        if nums[mid] >= target:\n",
        "            r = mid - 1\n",
        "        else:\n",
        "            l = mid + 1\n",
        "    return l\n",
        "\n",
        "def compute200thnearestneighbour(x, data):\n",
        "    dists = []\n",
        "    for val in data:\n",
        "      # computing distances\n",
        "        dist = np.sum((x - val) **2 )\n",
        "        if(len(dists) == 200 and dists[199] > dist):\n",
        "            l = int(lower_bound(dists, dist))\n",
        "            if l < 200 and l >= 0 and dists[l] > dist:\n",
        "                dists[l] = dist\n",
        "        else:\n",
        "            dists.append(dist)\n",
        "            dists.sort()\n",
        "\n",
        "# Dist 199 contains the distance of 200th nearest neighbour.\n",
        "    return dists[199]\n",
        "\n",
        "vectors.shape\n"
      ],
      "metadata": {
        "id": "2KARXya8nLS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the 200th nearest neighbour distance of some point the dataset:\n",
        "twohundrethneigh = []\n",
        "for val in vectors[:1000]:\n",
        "    twohundrethneigh.append( compute200thnearestneighbour(val, vectors[:1000]) )\n",
        "twohundrethneigh.sort()"
      ],
      "metadata": {
        "id": "my0uU4vspMUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting for the Elbow Method :\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.title(\"Elbow Method for Finding the right Eps hyperparameter\")\n",
        "plt.plot([x for x in range(len(twohundrethneigh))], twohundrethneigh)\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Distance of 200th Nearest Neighbour\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vy17gVVJpOmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model_dbs = DBSCAN(eps = 5, min_samples = minPts)\n",
        "model_dbs.fit(vectors)"
      ],
      "metadata": {
        "id": "77SGoPWEpQt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dbs = df\n",
        "df_dbs[\"DBS Cluster Label\"] = model_dbs.labels_\n",
        "df_dbs"
      ],
      "metadata": {
        "id": "lb770poipShl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "dendro=hierarchy.dendrogram(hierarchy.linkage(vectors,method='ward'))\n",
        "plt.axhline(y=20)"
      ],
      "metadata": {
        "id": "KGacCEejpU6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  #took n=3 from dendrogram curve\n",
        "Agg=cluster.fit_predict(vectors)"
      ],
      "metadata": {
        "id": "uUJOTBS6pWwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['AVG-W2V Clus Label'] = cluster.labels_\n",
        "df.head()"
      ],
      "metadata": {
        "id": "sBcmrzIIpXWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hier_df = df # Give the labels and group to count the number of data in each clusters.\n",
        "hier_df[\"Hierarchial Cluster Labels\"] = cluster.labels_\n",
        "hier_df.groupby([\"Hierarchial Cluster Labels\"])[\"Reviews\"].count()"
      ],
      "metadata": {
        "id": "BRB-CEdTpbUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "The distance between the data points and the centroids of the clusters itself serves as the basis for K-means clustering. For clustering based on density, DBSCAN is utilized. Here, the areas with the highest point densities are located and separated from void areas. As the name suggests, layering is a component of hierarchical clustering. Each data point is first taken into account as a separate cluster, and the two clusters closest to one another are then identified.\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "This assignment helped me a lot in revising various machine learning algorithms such as MultinominalNB, SVM, KNN, Decision tree,\n",
        "Random Forest,XGBoost, Word2Vec, BERT.For all these algorithms we are calculating the metrics as well.\n",
        "For the text clustering task on the Amazon reviews dataset, I have applied K-means, DBSCAN, Hierarchical\n",
        "clustering, Word2Vec, and BERT. Overall, in this exercise i have got a comprehensive hands-on experience with\n",
        "different machine learning algorithms for text analysis, reinforced understanding of cross-validation\n",
        "techniques, and emphasized the importance of choosing appropriate evaluation metrics based on the task at\n",
        "hand.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}